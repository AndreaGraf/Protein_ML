{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mkgendocs in /home/andrea/anaconda3/envs/prot_ml/lib/python3.9/site-packages (0.9.2)\n",
      "Requirement already satisfied: pyyaml in /home/andrea/anaconda3/envs/prot_ml/lib/python3.9/site-packages (from mkgendocs) (6.0)\n",
      "Requirement already satisfied: mako in /home/andrea/anaconda3/envs/prot_ml/lib/python3.9/site-packages (from mkgendocs) (1.2.4)\n",
      "Requirement already satisfied: six in /home/andrea/anaconda3/envs/prot_ml/lib/python3.9/site-packages (from mkgendocs) (1.16.0)\n",
      "Requirement already satisfied: astor in /home/andrea/anaconda3/envs/prot_ml/lib/python3.9/site-packages (from mkgendocs) (0.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/andrea/anaconda3/envs/prot_ml/lib/python3.9/site-packages (from mako->mkgendocs) (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mkgendocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ast\n",
    "import importlib\n",
    "import re\n",
    "import json, yaml\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Union, get_type_hints\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "from mkgendocs.gendocs import generate\n",
    "from mkgendocs.parse import GoogleDocString\n",
    "\n",
    "from mako.template import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/andrea/Code/ProteinModels'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the  file header\n",
    "script = '/home/andrea/Code/ProteinModels/protml/apps/train.py'\n",
    "repo_dir = '/home/andrea/Code/ProteinModels'\n",
    "mkdocs_dir = '/home/andrea/Code/ProteinModels/docs' \n",
    "\n",
    "intro_contents = defaultdict(dict)\n",
    "with open(script, 'r') as source:\n",
    "    tree = ast.parse(source.read())\n",
    "\n",
    "intros = []\n",
    "for child in ast.iter_child_nodes(tree):\n",
    "    try:\n",
    "        if isinstance(child, (ast.Expr)):\n",
    "            child.value.s\n",
    "            intros.append(child.value.s)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docstring = intros[0]\n",
    "docstring_parser = GoogleDocString(docstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'header': '',\n",
       "  'text': 'The *protml app* for training models to map protein sequences to their phenotype and\\ngenerative models for generate sequences with high functional scores  \\n',\n",
       "  'args': []},\n",
       " {'header': 'Examples',\n",
       "  'text': 'Train a supervised model:\\n    ```python\\n    python3 -m protml.apps.train experiment=supervised/train_base                 train_data= < PATH_TO_TRAINING_DATA >                val_data= < PATH_TO_VALIDATION_DATA > \\n    ```\\nOveride model parameters from the command line:\\n\\n    python3 -m protml.apps.train experiment=supervised/train_base                 train_data= < PATH_TO_TRAINING_DATA >                val_data= < PATH_TO_VALIDATION_DATA >                trainer.max_epochs=50000                model.encoder.model_params.hidden_layer_sizes=[100,100,100,100,100]\\\\ \\n            z_dim=10    \\n\\nTrain a generative model:\\n\\n    python3 -m protml.apps.train experiment=vae/train_base                 train_data=< PATH_TO_TRAINING_DATA >                val_data= <PATH_TO_VALIDATION_DATA >                trainer.max_epochs=1000\\n            \\nSpecify parameters that are not set in the config file with +PARAMETER=VALUE:\\n\\n    python3 -m protml.apps.train experiment=vae/train_base                 train_data=< PATH_TO_TRAINING_DATA >                val_data= <PATH_TO_VALIDATION_DATA >                trainer.max_epochs=1000                +datamodule.params.use_weights=True\\n                   \\n',\n",
       "  'args': []}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docstring_parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers, data = docstring_parser.markdown()\n",
    "'signature' in data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Train a supervised model:', '', 'python3 -m protml.apps.train experiment=supervised/train_base train_data= < PATH_TO_TRAINING_DATA > val_data= < PATH_TO_VALIDATION_DATA >', '', 'Overide model parameters from the command line:', '', 'python3 -m protml.apps.train experiment=supervised/train_base train_data= < PATH_TO_TRAINING_DATA > val_data= < PATH_TO_VALIDATION_DATA > trainer.max_epochs=50000 model.encoder.model_params.hidden_layer_sizes=[100,100,100,100,100]\\\\', 'z_dim=10', '', 'Train a generative model:', '', 'python3 -m protml.apps.train experiment=vae/train_base train_data=< PATH_TO_TRAINING_DATA > val_data= <PATH_TO_VALIDATION_DATA > trainer.max_epochs=1000', '', 'Specify parameters that are not set in the config file with +PARAMETER=VALUE:', '', 'python3 -m protml.apps.train experiment=vae/train_base train_data=< PATH_TO_TRAINING_DATA > val_data= <PATH_TO_VALIDATION_DATA > trainer.max_epochs=1000 +datamodule.params.use_weights=True', '', '']\n"
     ]
    }
   ],
   "source": [
    "format_md_text(data[1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#function to format the text section of the docstring data and extract code blocks\n",
    "def format_md_text(text):\n",
    "    lines = text.split('\\n')\n",
    "    python_lines = []\n",
    "\n",
    "    for i,line in enumerate(lines):\n",
    "        #cleanup lines\n",
    "        line = line.strip()\n",
    "        line = re.sub(r'\\s+', ' ', line)\n",
    "        \n",
    "        if re.match(r'^python( 3)? ', line):\n",
    "            line = f'```python\\n{line}\\n```'\n",
    "        lines[i] = line\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "        \n",
    "\n",
    "def data_to_markdown(data):\n",
    "    \"\"\"Converts the data from the docstring parser to markdown\"\"\"\n",
    "    markdown = []\n",
    "    for d in data:\n",
    "        if 'header' in d and len(d['header']) > 0:\n",
    "            markdown.append(f'### {d[\"header\"]}')\n",
    "        if 'signature' in d:\n",
    "            markdown.append(f'```python\\n {d[\"signature\"]}\\n```')\n",
    "        if 'description' in d:\n",
    "            markdown.append(d['description'])\n",
    "        if 'args' in d and len(d['args']) > 0:\n",
    "            markdown.append('### Arguments')\n",
    "            for arg in d['args']:\n",
    "                markdown.append(f'* **{arg[\"name\"]}** ({arg[\"type\"]}): {arg[\"description\"]}')\n",
    "        if 'returns' in d:\n",
    "            markdown.append('### Returns')\n",
    "            markdown.append(f'* **{d[\"returns\"][\"type\"]}**: {d[\"returns\"][\"description\"]}')\n",
    "        if 'raises' in d:\n",
    "            markdown.append('### Raises')\n",
    "            for r in d['raises']:\n",
    "                markdown.append(f'* **{r[\"type\"]}**: {r[\"description\"]}')\n",
    "        if 'attributes' in d:\n",
    "            markdown.append('### Attributes')\n",
    "            for attr in d['attributes']:\n",
    "                markdown.append(f'* **{attr[\"name\"]}** ({attr[\"type\"]}): {attr[\"description\"]}')\n",
    "        if 'text' in d:\n",
    "            markdown.append(format_md_text(d['text']))\n",
    "\n",
    "    return '\\nn'.join(markdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The *protml app* for training models to map protein sequences to their phenotype and\n",
      "generative models for generate sequences with high functional scores\n",
      "\n",
      "n### Examples\n",
      "nTrain a supervised model:\n",
      "\n",
      "python3 -m protml.apps.train experiment=supervised/train_base train_data= < PATH_TO_TRAINING_DATA > val_data= < PATH_TO_VALIDATION_DATA >\n",
      "\n",
      "Overide model parameters from the command line:\n",
      "\n",
      "python3 -m protml.apps.train experiment=supervised/train_base train_data= < PATH_TO_TRAINING_DATA > val_data= < PATH_TO_VALIDATION_DATA > trainer.max_epochs=50000 model.encoder.model_params.hidden_layer_sizes=[100,100,100,100,100]\\\n",
      "z_dim=10\n",
      "\n",
      "Train a generative model:\n",
      "\n",
      "python3 -m protml.apps.train experiment=vae/train_base train_data=< PATH_TO_TRAINING_DATA > val_data= <PATH_TO_VALIDATION_DATA > trainer.max_epochs=1000\n",
      "\n",
      "Specify parameters that are not set in the config file with +PARAMETER=VALUE:\n",
      "\n",
      "python3 -m protml.apps.train experiment=vae/train_base train_data=< PATH_TO_TRAINING_DATA > val_data= <PATH_TO_VALIDATION_DATA > trainer.max_epochs=1000 +datamodule.params.use_weights=True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "markdown_str = data_to_markdown(data)\n",
    "print(markdown_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCSTRING_TEMPLATE = \"\"\"\n",
    "## if we are processing a header function\n",
    "%if header['Examples']:\n",
    "${h3} .${header['Examples']}\n",
    "%endif \n",
    "\n",
    "%for section in sections:\n",
    "    %if section['header']:\n",
    "\n",
    "**${section['header']}**\n",
    "\n",
    "    %else:\n",
    "---\n",
    "    %endif\n",
    "    %if section['args']:\n",
    "        %for arg in section['args']:\n",
    "        %if arg['field']:\n",
    "* **${arg['field']}** ${arg['signature']} : ${arg['description']}\n",
    "        %else:\n",
    "* ${arg['description']}\n",
    "        %endif\n",
    "        %endfor\n",
    "    %endif\n",
    "${section['text']}\n",
    "%endfor\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_info = {'name': 'protml', 'signature': 'python3', 'docstring':docstring}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "docstring_template = DOCSTRING_TEMPLATE\n",
    "markdown_template = Template(text=docstring_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m markdown_str \u001b[39m=\u001b[39m markdown_template\u001b[39m.\u001b[39;49mrender(header\u001b[39m=\u001b[39;49mdata[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m      2\u001b[0m                                        source\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      3\u001b[0m                                        signature\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpython3\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m                                        sections\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m      5\u001b[0m                                        headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m      6\u001b[0m                                        h2\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m##\u001b[39;49m\u001b[39m'\u001b[39;49m, h3\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m###\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/prot_ml/lib/python3.9/site-packages/mako/template.py:439\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **data)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdata):\n\u001b[1;32m    428\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Render the output of this template as a string.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \n\u001b[1;32m    430\u001b[0m \u001b[39m    If the template specifies an output encoding, the string\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m \n\u001b[1;32m    438\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m runtime\u001b[39m.\u001b[39;49m_render(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallable_, args, data)\n",
      "File \u001b[0;32m~/anaconda3/envs/prot_ml/lib/python3.9/site-packages/mako/runtime.py:874\u001b[0m, in \u001b[0;36m_render\u001b[0;34m(template, callable_, args, data, as_unicode)\u001b[0m\n\u001b[1;32m    871\u001b[0m context\u001b[39m.\u001b[39m_outputting_as_unicode \u001b[39m=\u001b[39m as_unicode\n\u001b[1;32m    872\u001b[0m context\u001b[39m.\u001b[39m_set_with_template(template)\n\u001b[0;32m--> 874\u001b[0m _render_context(\n\u001b[1;32m    875\u001b[0m     template,\n\u001b[1;32m    876\u001b[0m     callable_,\n\u001b[1;32m    877\u001b[0m     context,\n\u001b[1;32m    878\u001b[0m     \u001b[39m*\u001b[39;49margs,\n\u001b[1;32m    879\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs_for_callable(callable_, data),\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[39mreturn\u001b[39;00m context\u001b[39m.\u001b[39m_pop_buffer()\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/anaconda3/envs/prot_ml/lib/python3.9/site-packages/mako/runtime.py:916\u001b[0m, in \u001b[0;36m_render_context\u001b[0;34m(tmpl, callable_, context, *args, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tmpl, template\u001b[39m.\u001b[39mDefTemplate):\n\u001b[1;32m    914\u001b[0m     \u001b[39m# if main render method, call from the base of the inheritance stack\u001b[39;00m\n\u001b[1;32m    915\u001b[0m     (inherit, lclcontext) \u001b[39m=\u001b[39m _populate_self_namespace(context, tmpl)\n\u001b[0;32m--> 916\u001b[0m     _exec_template(inherit, lclcontext, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n\u001b[1;32m    917\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     \u001b[39m# otherwise, call the actual rendering method specified\u001b[39;00m\n\u001b[1;32m    919\u001b[0m     (inherit, lclcontext) \u001b[39m=\u001b[39m _populate_self_namespace(context, tmpl\u001b[39m.\u001b[39mparent)\n",
      "File \u001b[0;32m~/anaconda3/envs/prot_ml/lib/python3.9/site-packages/mako/runtime.py:943\u001b[0m, in \u001b[0;36m_exec_template\u001b[0;34m(callable_, context, args, kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m         _render_error(template, context, e)\n\u001b[1;32m    942\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     callable_(context, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32mmemory:0x7fd76b388880:24\u001b[0m, in \u001b[0;36mrender_body\u001b[0;34m(context, **pageargs)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Examples'"
     ]
    }
   ],
   "source": [
    "markdown_str = markdown_template.render(header=data[1],\n",
    "                                       source=None,\n",
    "                                       signature='python3',\n",
    "                                       sections=data,\n",
    "                                       headers=headers,\n",
    "                                       h2='##', h3='###')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Description\n",
    "You should design a set of experiments in order to evaluate your proposed methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Experimental Strategy Outline\n",
    "\n",
    "\n",
    "I will follow the steps outlined below to develop and evaluate a prediction model. Due to limited time and compute resources, these steps are designed to illustrate the project outline, an rather than representing real optimization, and only a small subset of parameters are tested in each case.  \n",
    "\n",
    "i) use the simple model architecture of the MAVE_NN network as a baseline model to benchmark further development\n",
    " \n",
    " -  train on single protein datasets to verify the predictive power is similar to the published model\n",
    " \n",
    "- with the view of using a more complex encoder method to map sequences into latent space, extend the measurment models from 1 to more dimensions\n",
    "\n",
    "ii) Originally,  VAE models for proteins are trained on their probability in evolution. Here we check how a simple VAE model sampling from data distribution with $\\exp(-\\Delta\\DeltaG) structures latent representation, as a better starting point for training a regression head.\n",
    "\n",
    "iii) Test the performance and generalization of the baseline models for multiple sequence data \n",
    "\n",
    "iv) Calculate embedded representation of the sequences from a pretrained protein language model (*T5prot*).\n",
    "\n",
    "v) Use the embedding vectors of the  as input to the Simple Network model (instead of the one hot encoded sequences used in the reference case) and evaluate the performance\n",
    "\n",
    "vi) add the wt sequence as an additional input feature. Rationale: jointly embedding different proteins, will likely create representations where sequences with a single AA difference are located close together. Adding the wt sequence we can train the prediction model on the distance from the wt sequence in the embedded space instead. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/envs/protml/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import h5py\n",
    "import importlib\n",
    "import hydra\n",
    "import omegaconf\n",
    "from omegaconf import DictConfig\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models were implemented in protml and trained using the train app of the protml framework.\n",
    "\n",
    "The run commands for the different experiments are collected in this notebook, and run for a single epoch for illustration. Real model training was done with these on COLAB gpus and as background scripts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Training supervised single protein baseline model \n",
    "Model source code: protml/models/baseline_supervised.py\n",
    "\n",
    "adapted from https://github.com/jbkinney/mavenn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-09 21:25:23,053][__main__][INFO] - Instantiating datamodule <protml.dataloaders.SequenceDataModule>\n",
      "[2023-05-09 21:25:23,071][__main__][INFO] - Seeding with 12345\n",
      "Global seed set to 12345\n",
      "[2023-05-09 21:25:23,073][__main__][INFO] - Instantiating model <protml.models.ENC_M.factory>\n",
      "[2023-05-09 21:25:23,101][__main__][INFO] - Instantiating logger <pytorch_lightning.loggers.MLFlowLogger>\n",
      "[2023-05-09 21:25:23,209][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>\n",
      "[2023-05-09 21:25:23,213][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>\n",
      "[2023-05-09 21:25:23,214][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>\n",
      "[2023-05-09 21:25:23,214][__main__][INFO] - Instantiating trainer <pytorch_lightning.Trainer>\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[2023-05-09 21:25:23,250][__main__][INFO] - Starting training!\n",
      "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
      "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ encoder                  │ VariationalEncoder                │  186 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules   │ ModuleList                        │  184 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.0 │ Linear                            │  144 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.1 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.2 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.3 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.4 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_mean          │ Linear                            │  1.0 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_log_var       │ Linear                            │  1.0 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ measurement              │ Mave_Global_Epistasis_Measurement │    601 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ loss_function            │ GaussNLL_VAR_Loss                 │      1 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ loss_function.nll        │ GaussianNLLLoss                   │      0 │\n",
      "└────┴──────────────────────────┴───────────────────────────────────┴────────┘\n",
      "\u001b[1mTrainable params\u001b[0m: 187 K                                                         \n",
      "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \n",
      "\u001b[1mTotal params\u001b[0m: 187 K                                                             \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                       \n",
      "/home/andrea/anaconda3/envs/protml/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|██████| 9/9 [00:00<00:00, 33.85it/s, v_num=d723, train/loss=1.580]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 9/9 [00:00<00:00, 25.96it/s, v_num=d723, train/loss=1.580, val/\u001b[A\n",
      "Epoch 0: 100%|█| 9/9 [00:00<00:00, 25.90it/s, v_num=d723, train/loss=1.580, val/`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 9/9 [00:00<00:00, 25.17it/s, v_num=d723, train/loss=1.580, val/\n"
     ]
    }
   ],
   "source": [
    "#Supervised training with the same setup as in the MAVE-NN publication \n",
    "!python3 -m protml.apps.train experiment=supervised/train_base \\\n",
    "    train_data=Data/data_sets/df_train_rand_1MJC.csv val_data=Data/data_sets/df_val_rand_1MJC.csv \\\n",
    "        trainer.max_epochs=1 model.encoder.model_params.hidden_layer_sizes=[100,100,100,100,100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-09 21:26:06,702][__main__][INFO] - Instantiating datamodule <protml.dataloaders.SequenceDataModule>\n",
      "[2023-05-09 21:26:06,716][__main__][INFO] - Seeding with 12345\n",
      "Global seed set to 12345\n",
      "[2023-05-09 21:26:06,717][__main__][INFO] - Instantiating model <protml.models.ENC_M.factory>\n",
      "[2023-05-09 21:26:06,732][__main__][INFO] - Instantiating logger <pytorch_lightning.loggers.MLFlowLogger>\n",
      "[2023-05-09 21:26:06,814][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>\n",
      "[2023-05-09 21:26:06,817][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>\n",
      "[2023-05-09 21:26:06,817][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>\n",
      "[2023-05-09 21:26:06,818][__main__][INFO] - Instantiating trainer <pytorch_lightning.Trainer>\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[2023-05-09 21:26:06,851][__main__][INFO] - Starting training!\n",
      "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
      "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ encoder                  │ VariationalEncoder                │  186 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules   │ ModuleList                        │  184 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.0 │ Linear                            │  144 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.1 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.2 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.3 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.4 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_mean          │ Linear                            │  1.0 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_log_var       │ Linear                            │  1.0 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ measurement              │ Mave_Global_Epistasis_Measurement │    601 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ loss_function            │ GaussNLL_VAR_Loss                 │      1 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ loss_function.nll        │ GaussianNLLLoss                   │      0 │\n",
      "└────┴──────────────────────────┴───────────────────────────────────┴────────┘\n",
      "\u001b[1mTrainable params\u001b[0m: 187 K                                                         \n",
      "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \n",
      "\u001b[1mTotal params\u001b[0m: 187 K                                                             \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                       \n",
      "/home/andrea/anaconda3/envs/protml/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|██████| 9/9 [00:00<00:00, 29.68it/s, v_num=f592, train/loss=1.580]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 9/9 [00:00<00:00, 24.31it/s, v_num=f592, train/loss=1.580, val/\u001b[A\n",
      "Epoch 1: 100%|█| 9/9 [00:00<00:00, 33.14it/s, v_num=f592, train/loss=0.196, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 9/9 [00:00<00:00, 24.71it/s, v_num=f592, train/loss=0.196, val/\u001b[A\n",
      "Epoch 2: 100%|█| 9/9 [00:00<00:00, 29.81it/s, v_num=f592, train/loss=0.240, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 9/9 [00:00<00:00, 24.94it/s, v_num=f592, train/loss=0.240, val/\u001b[A\n",
      "Epoch 3: 100%|█| 9/9 [00:00<00:00, 31.93it/s, v_num=f592, train/loss=0.183, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 9/9 [00:00<00:00, 25.52it/s, v_num=f592, train/loss=0.183, val/\u001b[A\n",
      "Epoch 4: 100%|█| 9/9 [00:00<00:00, 35.23it/s, v_num=f592, train/loss=0.134, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 9/9 [00:00<00:00, 28.09it/s, v_num=f592, train/loss=0.134, val/\u001b[A\n",
      "Epoch 5: 100%|█| 9/9 [00:00<00:00, 31.21it/s, v_num=f592, train/loss=0.155, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 9/9 [00:00<00:00, 26.03it/s, v_num=f592, train/loss=0.155, val/\u001b[A\n",
      "Epoch 6: 100%|█| 9/9 [00:00<00:00, 30.43it/s, v_num=f592, train/loss=0.113, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 9/9 [00:00<00:00, 24.17it/s, v_num=f592, train/loss=0.113, val/\u001b[A\n",
      "Epoch 7: 100%|█| 9/9 [00:00<00:00, 36.20it/s, v_num=f592, train/loss=0.119, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 9/9 [00:00<00:00, 27.92it/s, v_num=f592, train/loss=0.119, val/\u001b[A\n",
      "Epoch 8: 100%|█| 9/9 [00:00<00:00, 36.48it/s, v_num=f592, train/loss=0.125, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 9/9 [00:00<00:00, 29.17it/s, v_num=f592, train/loss=0.125, val/\u001b[A\n",
      "Epoch 9: 100%|█| 9/9 [00:00<00:00, 32.67it/s, v_num=f592, train/loss=0.118, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 9/9 [00:00<00:00, 25.20it/s, v_num=f592, train/loss=0.118, val/\u001b[A\n",
      "Epoch 9: 100%|█| 9/9 [00:00<00:00, 25.15it/s, v_num=f592, train/loss=0.118, val/`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "Epoch 9: 100%|█| 9/9 [00:00<00:00, 25.09it/s, v_num=f592, train/loss=0.118, val/\n"
     ]
    }
   ],
   "source": [
    "!python3 -m protml.apps.train experiment=supervised/train_base \\\n",
    "    train_data=Data/data_sets/df_train_rand_1MJC.csv val_data=Data/data_sets/df_val_rand_1MJC.csv \\\n",
    "        trainer.max_epochs=10 model.encoder.model_params.hidden_layer_sizes=[100,100,100,100,100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-09 21:27:22,844][__main__][INFO] - Instantiating datamodule <protml.dataloaders.SequenceDataModule>\n",
      "[2023-05-09 21:27:22,858][__main__][INFO] - Seeding with 42\n",
      "Global seed set to 42\n",
      "[2023-05-09 21:27:22,859][__main__][INFO] - Instantiating model <protml.models.ENC_M.factory>\n",
      "[2023-05-09 21:27:22,876][__main__][INFO] - Instantiating logger <pytorch_lightning.loggers.MLFlowLogger>\n",
      "[2023-05-09 21:27:22,953][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>\n",
      "[2023-05-09 21:27:22,956][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>\n",
      "[2023-05-09 21:27:22,957][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>\n",
      "[2023-05-09 21:27:22,957][__main__][INFO] - Instantiating trainer <pytorch_lightning.Trainer>\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[2023-05-09 21:27:22,990][__main__][INFO] - Starting training!\n",
      "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
      "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ encoder                  │ VariationalEncoder                │  453 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules   │ ModuleList                        │  449 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.0 │ Linear                            │  288 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.1 │ Linear                            │ 40.2 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.2 │ Linear                            │ 40.2 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.3 │ Linear                            │ 40.2 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.4 │ Linear                            │ 40.2 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_mean          │ Linear                            │  2.0 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_log_var       │ Linear                            │  2.0 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ measurement              │ Mave_Global_Epistasis_Measurement │    601 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ loss_function            │ GaussNLL_VAR_Loss                 │      1 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ loss_function.nll        │ GaussianNLLLoss                   │      0 │\n",
      "└────┴──────────────────────────┴───────────────────────────────────┴────────┘\n",
      "\u001b[1mTrainable params\u001b[0m: 453 K                                                         \n",
      "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \n",
      "\u001b[1mTotal params\u001b[0m: 453 K                                                             \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                       \n",
      "/home/andrea/anaconda3/envs/protml/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|██████| 9/9 [00:00<00:00, 32.11it/s, v_num=9f9c, train/loss=0.173]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 9/9 [00:00<00:00, 24.32it/s, v_num=9f9c, train/loss=0.173, val/\u001b[A\n",
      "Epoch 1: 100%|█| 9/9 [00:00<00:00, 27.22it/s, v_num=9f9c, train/loss=0.409, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 9/9 [00:00<00:00, 22.24it/s, v_num=9f9c, train/loss=0.409, val/\u001b[A\n",
      "Epoch 2: 100%|█| 9/9 [00:00<00:00, 32.06it/s, v_num=9f9c, train/loss=0.111, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 9/9 [00:00<00:00, 26.52it/s, v_num=9f9c, train/loss=0.111, val/\u001b[A\n",
      "Epoch 3: 100%|█| 9/9 [00:00<00:00, 32.11it/s, v_num=9f9c, train/loss=0.198, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 9/9 [00:00<00:00, 20.90it/s, v_num=9f9c, train/loss=0.198, val/\u001b[A\n",
      "Epoch 4: 100%|█| 9/9 [00:00<00:00, 31.01it/s, v_num=9f9c, train/loss=0.119, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 9/9 [00:00<00:00, 24.45it/s, v_num=9f9c, train/loss=0.119, val/\u001b[A\n",
      "Epoch 5: 100%|█| 9/9 [00:00<00:00, 31.34it/s, v_num=9f9c, train/loss=0.133, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 9/9 [00:00<00:00, 24.17it/s, v_num=9f9c, train/loss=0.133, val/\u001b[A\n",
      "Epoch 6: 100%|█| 9/9 [00:00<00:00, 31.06it/s, v_num=9f9c, train/loss=0.105, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 9/9 [00:00<00:00, 24.59it/s, v_num=9f9c, train/loss=0.105, val/\u001b[A\n",
      "Epoch 7: 100%|█| 9/9 [00:00<00:00, 32.05it/s, v_num=9f9c, train/loss=0.104, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 9/9 [00:00<00:00, 24.39it/s, v_num=9f9c, train/loss=0.104, val/\u001b[A\n",
      "Epoch 8: 100%|█| 9/9 [00:00<00:00, 28.06it/s, v_num=9f9c, train/loss=0.0854, val\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 9/9 [00:00<00:00, 22.20it/s, v_num=9f9c, train/loss=0.0854, val\u001b[A\n",
      "Epoch 9: 100%|█| 9/9 [00:00<00:00, 26.87it/s, v_num=9f9c, train/loss=0.111, val/\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 9/9 [00:00<00:00, 21.74it/s, v_num=9f9c, train/loss=0.111, val/\u001b[A\n",
      "Epoch 10: 100%|█| 9/9 [00:00<00:00, 26.91it/s, v_num=9f9c, train/loss=0.107, val\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 9/9 [00:00<00:00, 22.58it/s, v_num=9f9c, train/loss=0.107, val\u001b[A\n",
      "Epoch 11: 100%|█| 9/9 [00:00<00:00, 27.21it/s, v_num=9f9c, train/loss=0.0874, va\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 9/9 [00:00<00:00, 22.23it/s, v_num=9f9c, train/loss=0.0874, va\u001b[A\n",
      "Epoch 12: 100%|█| 9/9 [00:00<00:00, 31.60it/s, v_num=9f9c, train/loss=0.103, val\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 9/9 [00:00<00:00, 24.78it/s, v_num=9f9c, train/loss=0.103, val\u001b[A\n",
      "Epoch 13: 100%|█| 9/9 [00:00<00:00, 26.61it/s, v_num=9f9c, train/loss=0.122, val\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 9/9 [00:00<00:00, 21.80it/s, v_num=9f9c, train/loss=0.122, val\u001b[A\n",
      "Epoch 14: 100%|█| 9/9 [00:00<00:00, 28.39it/s, v_num=9f9c, train/loss=0.102, val\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 9/9 [00:00<00:00, 23.02it/s, v_num=9f9c, train/loss=0.102, val\u001b[A\n",
      "Epoch 15: 100%|█| 9/9 [00:00<00:00, 27.16it/s, v_num=9f9c, train/loss=0.0969, va\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 9/9 [00:00<00:00, 21.56it/s, v_num=9f9c, train/loss=0.0969, va\u001b[A\n",
      "Epoch 16: 100%|█| 9/9 [00:00<00:00, 14.42it/s, v_num=9f9c, train/loss=0.108, val\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 9/9 [00:00<00:00, 12.49it/s, v_num=9f9c, train/loss=0.108, val\u001b[A\n",
      "Epoch 17: 100%|█| 9/9 [00:00<00:00, 20.56it/s, v_num=9f9c, train/loss=0.112, val\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 9/9 [00:00<00:00, 17.42it/s, v_num=9f9c, train/loss=0.112, val\u001b[A\n",
      "Epoch 18: 100%|█| 9/9 [00:00<00:00, 27.78it/s, v_num=9f9c, train/loss=0.124, val\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 9/9 [00:00<00:00, 21.44it/s, v_num=9f9c, train/loss=0.124, val\u001b[A\n",
      "Epoch 19: 100%|█| 9/9 [00:00<00:00, 28.82it/s, v_num=9f9c, train/loss=0.118, val\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 9/9 [00:00<00:00, 22.18it/s, v_num=9f9c, train/loss=0.118, val\u001b[A\n",
      "Epoch 19: 100%|█| 9/9 [00:00<00:00, 22.13it/s, v_num=9f9c, train/loss=0.118, val`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "Epoch 19: 100%|█| 9/9 [00:00<00:00, 22.09it/s, v_num=9f9c, train/loss=0.118, val\n"
     ]
    }
   ],
   "source": [
    "#increased layer size \n",
    "!python3 -m protml.apps.train experiment=supervised/train_base \\\n",
    "    train_data=Data/data_sets/df_train_rand_1MJC.csv val_data=Data/data_sets/df_val_rand_1MJC.csv \\\n",
    "        trainer.max_epochs=20 model.encoder.model_params.hidden_layer_sizes=[200,200,200,200,200] z_dim=10 seed=42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a more indepth analysis, many more model options and architecturs could be evaluated easily, by looking more extensively at different parameter combinations, and bringing ray tune into the framework. One would also look at other single protein sets. The system is well suited for parameter optimization, as the protein sizes are small and training runs fast.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Train a baseline VAE model with free energy based probability sampling\n",
    "model source code: protml/models/baseline_vae.py\n",
    "\n",
    "\n",
    "*Adapted from the EVE model: https://github.com/OATML/EVE* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-09 22:14:02,719][__main__][INFO] - Instantiating datamodule <protml.dataloaders.SequenceDataModule>\n",
      "[2023-05-09 22:14:02,750][__main__][INFO] - Seeding with 42\n",
      "Global seed set to 42\n",
      "[2023-05-09 22:14:02,752][__main__][INFO] - Instantiating model <protml.models.VAE>\n",
      "[2023-05-09 22:14:02,846][__main__][INFO] - Instantiating logger <pytorch_lightning.loggers.MLFlowLogger>\n",
      "[2023-05-09 22:14:02,997][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>\n",
      "[2023-05-09 22:14:03,002][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>\n",
      "[2023-05-09 22:14:03,003][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>\n",
      "[2023-05-09 22:14:03,004][__main__][INFO] - Instantiating trainer <pytorch_lightning.Trainer>\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[2023-05-09 22:14:03,068][__main__][INFO] - Starting training!\n",
      "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType              \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
      "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ encoder                  │ VariationalEncoder │  2.0 M │\n",
      "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules   │ ModuleList         │  2.0 M │\n",
      "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.0 │ Linear             │  1.4 M │\n",
      "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.1 │ Linear             │  500 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.2 │ Linear             │ 50.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_mean          │ Linear             │  5.0 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_log_var       │ Linear             │  5.0 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ decoder                  │ MLPDecoder         │  2.0 M │\n",
      "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ decoder.hidden_layers    │ ModuleList         │  556 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ decoder.hidden_layers.0  │ Linear             │  5.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ decoder.hidden_layers.1  │ Linear             │ 50.5 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ decoder.hidden_layers.2  │ Linear             │  501 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m12\u001b[0m\u001b[2m \u001b[0m│ decoder.dropout_layer    │ Dropout            │      0 │\n",
      "└────┴──────────────────────────┴────────────────────┴────────┘\n",
      "\u001b[1mTrainable params\u001b[0m: 4.0 M                                                         \n",
      "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \n",
      "\u001b[1mTotal params\u001b[0m: 4.0 M                                                             \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 15                                      \n",
      "/home/andrea/anaconda3/envs/protml/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|██████| 9/9 [00:00<00:00, 11.15it/s, v_num=7f44, train/loss=474.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 9/9 [00:00<00:00,  9.19it/s, v_num=7f44, train/loss=474.0, val/\u001b[A\n",
      "Epoch 0: 100%|█| 9/9 [00:00<00:00,  9.18it/s, v_num=7f44, train/loss=474.0, val/\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 9/9 [00:01<00:00,  8.44it/s, v_num=7f44, train/loss=474.0, val/\n",
      "[2023-05-09 22:14:09,887][__main__][INFO] - Instantiating datamodule <protml.dataloaders.SequenceDataModule>\n",
      "[2023-05-09 22:14:09,911][__main__][INFO] - Seeding with 42\n",
      "Global seed set to 42\n",
      "[2023-05-09 22:14:09,912][__main__][INFO] - Instantiating model <protml.models.VAE>\n",
      "[2023-05-09 22:14:09,975][__main__][INFO] - Instantiating logger <pytorch_lightning.loggers.MLFlowLogger>\n",
      "[2023-05-09 22:14:10,128][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>\n",
      "[2023-05-09 22:14:10,133][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>\n",
      "[2023-05-09 22:14:10,134][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>\n",
      "[2023-05-09 22:14:10,134][__main__][INFO] - Instantiating trainer <pytorch_lightning.Trainer>\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[2023-05-09 22:14:10,195][__main__][INFO] - Starting training!\n",
      "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType              \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
      "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ encoder                  │ VariationalEncoder │  2.0 M │\n",
      "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules   │ ModuleList         │  2.0 M │\n",
      "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.0 │ Linear             │  1.4 M │\n",
      "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.1 │ Linear             │  500 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.2 │ Linear             │ 50.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_mean          │ Linear             │  1.0 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_log_var       │ Linear             │  1.0 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ decoder                  │ MLPDecoder         │  2.0 M │\n",
      "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ decoder.hidden_layers    │ ModuleList         │  552 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ decoder.hidden_layers.0  │ Linear             │  1.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ decoder.hidden_layers.1  │ Linear             │ 50.5 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ decoder.hidden_layers.2  │ Linear             │  501 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m12\u001b[0m\u001b[2m \u001b[0m│ decoder.dropout_layer    │ Dropout            │      0 │\n",
      "└────┴──────────────────────────┴────────────────────┴────────┘\n",
      "\u001b[1mTrainable params\u001b[0m: 4.0 M                                                         \n",
      "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \n",
      "\u001b[1mTotal params\u001b[0m: 4.0 M                                                             \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 15                                      \n",
      "/home/andrea/anaconda3/envs/protml/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|██████| 9/9 [00:01<00:00,  8.98it/s, v_num=838e, train/loss=292.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 9/9 [00:01<00:00,  7.74it/s, v_num=838e, train/loss=292.0, val/\u001b[A\n",
      "Epoch 0: 100%|█| 9/9 [00:01<00:00,  7.73it/s, v_num=838e, train/loss=292.0, val/\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 9/9 [00:01<00:00,  7.19it/s, v_num=838e, train/loss=292.0, val/\n"
     ]
    }
   ],
   "source": [
    "#Training a basic  VAE with a latent spce dimension of z=50 as recomended in the EVE pubpicatin, as well as z=10 to make it compatible with \n",
    "#the MAVE model that is designed for a low dimensional representation. \n",
    "\n",
    "# !Note: the datamodule parameter: +datamodule.params.use_weights=True switches on the sample weights to exp(-DDG). \n",
    "\n",
    "!python3 -m protml.apps.train experiment=vae/train_base \\\n",
    "    train_data=Data/data_sets/df_train_rand_1MJC.csv val_data=Data/data_sets/df_val_rand_1MJC.csv \\\n",
    "        trainer.max_epochs=1 model.optimizer.lr=1e-4 model.encoder.model_params.hidden_layer_sizes=[1000,500,100]\\\n",
    "        model.decoder.model_params.hidden_layer_sizes=[100,500,1000] z_dim=50 seed=42 +datamodule.params.use_weights=True\\\n",
    "          callbacks.early_stopping.patience=1000\n",
    "        \n",
    "!python3 -m protml.apps.train experiment=vae/train_base \\\n",
    "    train_data=Data/data_sets/df_train_rand_1MJC.csv val_data=Data/data_sets/df_val_rand_1MJC.csv \\\n",
    "        trainer.max_epochs=1 model.optimizer.lr=1e-4 model.encoder.model_params.hidden_layer_sizes=[1000,500,100]\\\n",
    "        model.decoder.model_params.hidden_layer_sizes=[100,500,1000] z_dim=10 seed=42 +datamodule.params.use_weights=True\\\n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Testing generalization of the baseline Model to the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-09 22:30:15,998][__main__][INFO] - Instantiating datamodule <protml.dataloaders.SequenceDataModule>\n",
      "[2023-05-09 22:30:16,029][__main__][INFO] - Seeding with 12345\n",
      "Global seed set to 12345\n",
      "[2023-05-09 22:30:16,031][__main__][INFO] - Instantiating model <protml.models.ENC_M.factory>\n",
      "[2023-05-09 22:30:16,063][__main__][INFO] - Instantiating logger <pytorch_lightning.loggers.MLFlowLogger>\n",
      "[2023-05-09 22:30:16,237][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>\n",
      "[2023-05-09 22:30:16,242][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>\n",
      "[2023-05-09 22:30:16,243][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>\n",
      "[2023-05-09 22:30:16,244][__main__][INFO] - Instantiating trainer <pytorch_lightning.Trainer>\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[2023-05-09 22:30:16,313][__main__][INFO] - Starting training!\n",
      "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
      "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ encoder                  │ VariationalEncoder                │  184 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules   │ ModuleList                        │  184 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.0 │ Linear                            │  144 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.1 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.2 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.3 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.4 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_mean          │ Linear                            │    101 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_log_var       │ Linear                            │    101 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ measurement              │ Mave_Global_Epistasis_Measurement │    151 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ loss_function            │ GaussNLL_VAR_Loss                 │      1 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ loss_function.nll        │ GaussianNLLLoss                   │      0 │\n",
      "└────┴──────────────────────────┴───────────────────────────────────┴────────┘\n",
      "\u001b[1mTrainable params\u001b[0m: 184 K                                                         \n",
      "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \n",
      "\u001b[1mTotal params\u001b[0m: 184 K                                                             \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                       \n",
      "Epoch 0: 100%|█| 1049/1049 [01:10<00:00, 14.91it/s, v_num=140a, train/loss=0.200\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/132 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/132 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 1/132 [00:00<00:02, 59.43it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 2/132 [00:00<00:03, 43.12it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▍                 | 3/132 [00:00<00:02, 45.21it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 4/132 [00:00<00:02, 42.89it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 5/132 [00:00<00:02, 42.82it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▊                 | 6/132 [00:00<00:02, 43.26it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 7/132 [00:00<00:02, 42.42it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 8/132 [00:00<00:02, 43.36it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏                | 9/132 [00:00<00:02, 41.47it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 10/132 [00:00<00:02, 40.70it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 11/132 [00:00<00:02, 41.15it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 12/132 [00:00<00:02, 41.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 13/132 [00:00<00:02, 41.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 14/132 [00:00<00:02, 42.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 15/132 [00:00<00:02, 42.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 16/132 [00:00<00:02, 42.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 17/132 [00:00<00:02, 42.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▎              | 18/132 [00:00<00:02, 42.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 19/132 [00:00<00:02, 40.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 20/132 [00:00<00:02, 40.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 21/132 [00:00<00:02, 41.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 22/132 [00:00<00:02, 41.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 23/132 [00:00<00:02, 41.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 24/132 [00:00<00:02, 42.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 25/132 [00:00<00:02, 43.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 26/132 [00:00<00:02, 43.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 27/132 [00:00<00:02, 44.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 28/132 [00:00<00:02, 44.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 29/132 [00:00<00:02, 44.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▊             | 30/132 [00:00<00:02, 45.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 31/132 [00:00<00:02, 45.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 32/132 [00:00<00:02, 45.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 33/132 [00:00<00:02, 45.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 34/132 [00:00<00:02, 45.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 35/132 [00:00<00:02, 45.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 36/132 [00:00<00:02, 45.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 37/132 [00:00<00:02, 45.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 38/132 [00:00<00:02, 45.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 39/132 [00:00<00:02, 45.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▏           | 40/132 [00:00<00:02, 45.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 41/132 [00:01<00:02, 39.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 42/132 [00:01<00:02, 38.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▌           | 43/132 [00:01<00:02, 38.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 44/132 [00:01<00:02, 39.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 45/132 [00:01<00:02, 39.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 46/132 [00:01<00:02, 39.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 47/132 [00:01<00:02, 39.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 48/132 [00:01<00:02, 39.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 49/132 [00:01<00:02, 38.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 50/132 [00:01<00:02, 38.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 51/132 [00:01<00:02, 39.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 52/132 [00:01<00:02, 39.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 53/132 [00:01<00:02, 39.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 54/132 [00:01<00:01, 39.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████          | 55/132 [00:01<00:01, 39.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 56/132 [00:01<00:01, 39.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 57/132 [00:01<00:01, 39.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 58/132 [00:01<00:01, 39.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 59/132 [00:01<00:01, 39.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 60/132 [00:01<00:01, 39.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 61/132 [00:01<00:01, 39.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|███████▉         | 62/132 [00:01<00:01, 39.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 63/132 [00:01<00:01, 39.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 64/132 [00:01<00:01, 38.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 65/132 [00:01<00:01, 37.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 66/132 [00:01<00:01, 37.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 67/132 [00:01<00:01, 38.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 68/132 [00:01<00:01, 38.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 69/132 [00:01<00:01, 38.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|█████████        | 70/132 [00:01<00:01, 38.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 71/132 [00:01<00:01, 38.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 72/132 [00:01<00:01, 38.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 73/132 [00:01<00:01, 38.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 74/132 [00:01<00:01, 37.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 75/132 [00:01<00:01, 37.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 76/132 [00:02<00:01, 37.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▉       | 77/132 [00:02<00:01, 37.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 78/132 [00:02<00:01, 38.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 79/132 [00:02<00:01, 38.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 80/132 [00:02<00:01, 38.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 81/132 [00:02<00:01, 38.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 82/132 [00:02<00:01, 38.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 83/132 [00:02<00:01, 38.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▊      | 84/132 [00:02<00:01, 38.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▉      | 85/132 [00:02<00:01, 37.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████      | 86/132 [00:02<00:01, 37.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|███████████▏     | 87/132 [00:02<00:01, 36.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|███████████▎     | 88/132 [00:02<00:01, 36.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|███████████▍     | 89/132 [00:02<00:01, 36.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|███████████▌     | 90/132 [00:02<00:01, 36.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████▋     | 91/132 [00:02<00:01, 36.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▊     | 92/132 [00:02<00:01, 36.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▉     | 93/132 [00:02<00:01, 36.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████     | 94/132 [00:02<00:01, 36.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▏    | 95/132 [00:02<00:01, 36.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|████████████▎    | 96/132 [00:02<00:00, 36.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|████████████▍    | 97/132 [00:02<00:00, 36.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|████████████▌    | 98/132 [00:02<00:00, 36.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████▊    | 99/132 [00:02<00:00, 36.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████    | 100/132 [00:02<00:00, 36.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▏   | 101/132 [00:02<00:00, 36.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 102/132 [00:02<00:00, 37.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 103/132 [00:02<00:00, 37.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▌   | 104/132 [00:02<00:00, 37.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 105/132 [00:02<00:00, 37.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 106/132 [00:02<00:00, 37.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 107/132 [00:02<00:00, 37.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 108/132 [00:02<00:00, 37.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 109/132 [00:02<00:00, 37.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 110/132 [00:02<00:00, 37.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 111/132 [00:02<00:00, 37.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 112/132 [00:02<00:00, 37.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▋  | 113/132 [00:02<00:00, 37.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 114/132 [00:03<00:00, 37.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 115/132 [00:03<00:00, 37.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 116/132 [00:03<00:00, 37.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 117/132 [00:03<00:00, 37.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 118/132 [00:03<00:00, 37.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 119/132 [00:03<00:00, 37.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 120/132 [00:03<00:00, 37.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 121/132 [00:03<00:00, 37.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 122/132 [00:03<00:00, 37.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 123/132 [00:03<00:00, 37.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 124/132 [00:03<00:00, 37.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 125/132 [00:03<00:00, 37.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▎| 126/132 [00:03<00:00, 37.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 127/132 [00:03<00:00, 37.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 128/132 [00:03<00:00, 37.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 129/132 [00:03<00:00, 37.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▊| 130/132 [00:03<00:00, 37.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 131/132 [00:03<00:00, 37.35it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 1049/1049 [01:14<00:00, 14.15it/s, v_num=140a, train/loss=0.200\u001b[A\n",
      "Epoch 0: 100%|█| 1049/1049 [01:14<00:00, 14.15it/s, v_num=140a, train/loss=0.200\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 1049/1049 [01:14<00:00, 14.14it/s, v_num=140a, train/loss=0.200\n"
     ]
    }
   ],
   "source": [
    "!python3 -m protml.apps.train experiment=supervised/train_base \\\n",
    "    train_data=Data/data_sets/train_data_rand.csv val_data=Data/data_sets/val_data_rand.csv \\\n",
    "        trainer.max_epochs=1 model.encoder.model_params.hidden_layer_sizes=[100,100,100,100,100] \\\n",
    "         z_dim=1 callbacks.early_stopping.patience=500  callbacks.model_checkpoint.save_last=last  \n",
    "\n",
    "!python3 -m protml.apps.train experiment=supervised/train_base \\\n",
    "    train_data=Data/data_sets/train_data_rand.csv val_data=Data/data_sets/val_data_rand.csv \\\n",
    "        trainer.max_epochs=500 model.encoder.model_params.hidden_layer_sizes=[100,100,100,100,100] z_dim=10 seed=42 trainer=lightning/gpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv) Calculating the sequence embedding vectors from the pretrained T5Prot protein language model\n",
    "\n",
    "! Use a GPU for this part. It is computationally expensive. \n",
    "\n",
    "ProtTrans Language Model: https://github.com/agemagician/ProtTrans\n",
    "\n",
    "Scripts and functions are adapted from https://github.com/Rostlab/VESPA\n",
    "\n",
    "... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies and check whether GPU is available. \n",
    "from transformers import T5EncoderModel, T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "#ensure that GPU is available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using {}\".format(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the functions for generating the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ProtT5 in half-precision (more specifically: the encoder-part of ProtT5-XL-U50) \n",
    "def get_T5_model():\n",
    "    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
    "    model = model.to(device) # move model to GPUif available\n",
    "    #model = model.eval() # set model to evaluation model\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def get_embeddings( model, tokenizer, seqs:list[tuple], per_residue:bool, per_protein:bool, max_length = 73, \n",
    "                   max_residues=4000, max_seq_len=1000, max_batch=100 ) -> dict:\n",
    "    \"\"\"  Generate embeddings via batch-processing\n",
    "\n",
    "    Args:\n",
    "        model: pretrained model\n",
    "        tokenizer: T5model Tokeniser\n",
    "        seqs (list): Tuples mapping of unique identifier to sequence\n",
    "        per_residue (bool): indicates that embeddings for each residue in a protein should be returned.\n",
    "        per_protein (bool): indicates that embeddings for a whole protein should be returned (average-pooling)\n",
    "        max_length (int, optional): _description_. Defaults to 73.\n",
    "        max_residues (int, optional): max_residues gives the upper limit of residues within one batch.\n",
    "        max_seq_len (int, optional): maximum_sequence length. Defaults to 1000.\n",
    "        max_batch (int, optional): _max_batch gives the upper number of sequences per batch. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        dict: embedding results\n",
    "    \"\"\"\n",
    "\n",
    "    results = {\"residue_embs\" : dict(), \n",
    "               \"protein_embs\" : dict(),\n",
    "               }\n",
    "    #already know max length from previous preprocessing\n",
    "    # sort sequences according to length (reduces unnecessary padding --> speeds up embedding) ...already done that\n",
    "    #seq_dict   = sorted( seqs.items(), key=lambda kv: len( seqs[kv[0]] ), reverse=True )\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    batch = list()\n",
    "    for seq_idx, (pdb_id, seq) in enumerate(seqs,1):\n",
    "        seq = seq\n",
    "        seq_len = len(seq)\n",
    "        seq = ' '.join(list(seq))\n",
    "        batch.append((pdb_id,seq,seq_len))\n",
    "\n",
    "        # count residues in current batch and add the last sequence length to\n",
    "        # avoid that batches with (n_res_batch > max_residues) get processed \n",
    "        n_res_batch = sum([ s_len for  _, _, s_len in batch ]) + seq_len \n",
    "        if len(batch) >= max_batch or n_res_batch>=max_residues or seq_idx==len(seqs) or seq_len>max_seq_len:\n",
    "            pdb_ids, seqs, seq_lens = zip(*batch)\n",
    "            batch = list()\n",
    "\n",
    "            # add_special_tokens adds extra token at the end of each sequence - this requires a max length of sequence length+1\n",
    "            token_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding='max_length', max_length= max_length)\n",
    "            input_ids      = torch.tensor(token_encoding['input_ids']).to(device)\n",
    "            attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
    "            \n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    # returns: ( batch-size x max_seq_len_in_minibatch x embedding_dim )\n",
    "                    embedding_repr = model(input_ids, attention_mask=attention_mask)\n",
    "            except RuntimeError:\n",
    "                print(\"RuntimeError during embedding for {} (L={})\".format(pdb_id, seq_len))\n",
    "                continue\n",
    "\n",
    "            for batch_idx, identifier in enumerate(pdb_ids): # for each protein in the current mini-batch\n",
    "                #f batch_idx % 100 ==0:\n",
    "                #print('processig batch ', batch_idx )\n",
    "                s_len = seq_lens[batch_idx]\n",
    "                emb = embedding_repr.last_hidden_state[batch_idx,:]\n",
    "               \n",
    "                if per_residue: # store per-residue embeddings (Lx1024)\n",
    "                    results[\"residue_embs\"][ identifier ] = emb.detach().cpu().numpy().squeeze()\n",
    "                if per_protein: # apply average-pooling to derive per-protein embeddings (1024-d)\n",
    "                    protein_emb = emb[:s_len].mean(dim=0)\n",
    "                    results[\"protein_embs\"][identifier] = protein_emb.detach().cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "    passed_time=time.time()-start\n",
    "    avg_time = passed_time/len(results[\"residue_embs\"]) if per_residue else passed_time/len(results[\"protein_embs\"])\n",
    "    print('\\n############# EMBEDDING STATS #############')\n",
    "    print('Total number of per-residue embeddings: {}'.format(len(results[\"residue_embs\"])))\n",
    "    print('Total number of per-protein embeddings: {}'.format(len(results[\"protein_embs\"])))\n",
    "    print(\"Time for generating embeddings: {:.1f}[m] ({:.3f}[s/protein])\".format(\n",
    "        passed_time/60, avg_time ))\n",
    "    print('\\n############# END #############')\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_protein_batches(df:pd.DataFrame, protein_ids:list[str])->dict:\n",
    "    \"\"\"generates single protein input batches, that can be processed and saved in .h5 file format.\n",
    "        Required because for processing large batches RAM limit is quickly exceeded.  \n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): input dataframe\n",
    "        protein_ids (list[str]): list of protein name str\n",
    "\n",
    "    Returns:\n",
    "        dict: mapping protein names to sequence data\n",
    "    \"\"\"\n",
    "    seq_lists ={}\n",
    "    for pid in protein_ids:\n",
    "        seq_dl = []\n",
    "        #create unique id for each sequ and add to datalist  \n",
    "        for i,r in df[df.pdbid == pid].iloc[:].iterrows(): \n",
    "            seq_dl.append((r.pdbid +'_'+ str(i), r.seq))\n",
    "        seq_lists[pid] = seq_dl\n",
    "    return seq_lists\n",
    "\n",
    " \n",
    "def save_embeddings(output_path:str, emb_dict:dict, saving_pattern = 'a')->None:\n",
    "    \"\"\"append results to .h5 lookup file, to free memory\n",
    "\n",
    "    Args:\n",
    "        output_path (str): h5 file\n",
    "        emb_dict (dict): results from get_embeddings\n",
    "        saving_pattern (str, optional): Defaults to 'a'.\n",
    "    \"\"\"\n",
    "    with h5py.File(output_path, saving_pattern) as hf:\n",
    "        for sequence_id, embedding in emb_dict.items():\n",
    "            hf.create_dataset(sequence_id, data=embedding)\n",
    "  \n",
    "\n",
    "def compute_embedding(model, tokenizer, seq_dict:dict, output_path:str) ->None:\n",
    "    \"\"\"compute embeddings for all proteins in seq_dict\n",
    "\n",
    "    Args:\n",
    "        model (_T5model_): pretrained model\n",
    "        tokenizer (T5 Tokenizer): Tokenizer\n",
    "        seq_dict (dict): input data. protein_id: seq list\n",
    "        output_path (str): h5 file\n",
    "    \"\"\"\n",
    "    for prot, seqs in seq_dict.items():\n",
    "        print('processing protein:' , prot )\n",
    "        results = get_embeddings(model.eval(), tokenizer, seqs=seqs, per_protein=True, per_residue=True, max_length=73)\n",
    "    \n",
    "        save_embeddings(os.path.join(output_path,\"residue_embeddings1.h5\"), results[\"residue_embs\"])\n",
    "        save_embeddings(os.path.join(output_path,\"protein_embeddings1.h5\"), results[\"protein_embs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the input data for embedding\n",
    "data_dir = 'Data/data_sets/'\n",
    "\n",
    "filename = 'train_data_prot.csv'\n",
    "df = pd.read_csv(os.path.join(data_dir,filename), index_col=0)\n",
    "\n",
    "filename = 'val_data_prot.csv'\n",
    "df_v = pd.read_csv(os.path.join(data_dir,filename), index_col=0)\n",
    "\n",
    "filename = 'test_data_prot.csv'\n",
    "df_test = pd.read_csv(os.path.join(data_dir,filename), index_col=0)\n",
    "\n",
    "filename = 'test_proteins_data.csv'\n",
    "df_ho = pd.read_csv(os.path.join(data_dir,filename), index_col=0)\n",
    "\n",
    "#strip padding for embedding\n",
    "df['seq'] = df.seq.apply(lambda x: x.strip('-'))\n",
    "df_v['seq'] = df_v.seq.apply(lambda x: x.strip('-'))\n",
    "df_test['seq'] = df_test.seq.apply(lambda x: x.strip('-'))\n",
    "df_ho['seq'] = df_ho.seq.apply(lambda x: x.strip('-'))\n",
    "\n",
    "protein_ids = list(df.pdbid.unique())\n",
    "protein_v_ids = list(df_v.pdbid.unique())\n",
    "protein_test_ids = list(df_test.pdbid.unique())\n",
    "protein_ho_ids = list(df_ho.pdbid.unique())\n",
    "\n",
    "seq_lists = create_protein_batches(df, protein_ids)\n",
    "seq_lists1 = create_protein_batches(df_v, protein_v_ids)\n",
    "seq_lists2 = create_protein_batches(df_test, protein_test_ids)\n",
    "seq_lists3 = create_protein_batches(df_ho, protein_ho_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = get_T5_model()\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the embeddings -- use GPU--\n",
    "compute_embedding(model.eval(),tokenizer, seq_lists, 'drive/MyDrive/protT5/output')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############# EMBEDDING STATS #############\n",
      "Total number of per-residue embeddings: 304\n",
      "Total number of per-protein embeddings: 304\n",
      "Time for generating embeddings: 3.1[m] (0.605[s/protein])\n",
      "\n",
      "############# END #############\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#similarly also get the embeddings for the wt sequences\n",
    "\n",
    "#first load wildtype dictionary, that was saved in the data_preprocessing steps\n",
    "j_file = \"Data/wt_sequences.json\"\n",
    "\n",
    "with open(j_file, \"r\") as fj:\n",
    "    wt_dict = json.load(fj)\n",
    "fj.close()\n",
    "wt_list = [(k, seq) for k, seq in wt_dict.items()]\n",
    "\n",
    "# get the embedding vectors\n",
    "wt_emb = get_embeddings(model.eval(), tokenizer, seqs=wt_list, per_protein=True, per_residue=True, max_length=73)\n",
    "save_embeddings('protT5/output/wt_embeddings.h5' , wt_emb['residue_embs'], 'a', )\n",
    "ssave_embeddings('protT5/output/wt_embeddings.h5' , wt_emb['residue_embs'], 'a', )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save computation, the calculated embedding files (protein_embeddings.h5, residue_embeddings.h5, wt_embeddings.h5, wt_prot_embeddings.h5) are provided along with the code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v) Training models with pretrained embeddings representation of the input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models source code: \n",
    "    - protml/models/baseline_supervised \n",
    "    - protml/models/dz_supervised \n",
    "\n",
    "datamodule: \n",
    "    - ddg data. \n",
    "    \n",
    "The pretrained embedding arrays for the whole datasets do not fit into memory. As a workaround the datamodule saves the list of lookup keys as data. \n",
    "A custom torch dataset is defined, which retrieves only the embedding arrays for the current batch from .h5 in the __getitem__ method:\n",
    "\n",
    "\n",
    " ```python\n",
    " EmbeddingsDataset(TensorDataset) \n",
    " ```\n",
    " \n",
    "```python\n",
    "def __getitem__(self, index):\n",
    "        \n",
    "        #lookup embedding in .h5\n",
    "        pid = self.ids[index]\n",
    "        embedding = np.array(self.embeddings[pid])\n",
    "        embedding = embedding[:-1,:]\n",
    "\n",
    "        return tuple([embedding] + [tensor[index] for tensor in self.tensors]) \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-10 01:01:06,722][__main__][INFO] - Instantiating datamodule <protml.dataloaders.EmbeddingsDataModule>\n",
      "[2023-05-10 01:01:07,443][__main__][INFO] - Seeding with 12345\n",
      "Global seed set to 12345\n",
      "[2023-05-10 01:01:07,445][__main__][INFO] - Instantiating model <protml.models.ENC_M.factory>\n",
      "[2023-05-10 01:01:07,498][__main__][INFO] - Instantiating logger <pytorch_lightning.loggers.MLFlowLogger>\n",
      "[2023-05-10 01:01:07,591][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>\n",
      "[2023-05-10 01:01:07,594][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>\n",
      "[2023-05-10 01:01:07,595][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>\n",
      "[2023-05-10 01:01:07,595][__main__][INFO] - Instantiating trainer <pytorch_lightning.Trainer>\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[2023-05-10 01:01:07,630][__main__][INFO] - Starting training!\n",
      "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
      "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ encoder                  │ VariationalEncoder                │  7.4 M │\n",
      "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules   │ ModuleList                        │  7.4 M │\n",
      "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.0 │ Linear                            │  7.4 M │\n",
      "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.1 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.2 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.3 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.4 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_mean          │ Linear                            │    101 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_log_var       │ Linear                            │    101 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ measurement              │ Mave_Global_Epistasis_Measurement │    151 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ loss_function            │ GaussNLL_VAR_Loss                 │      1 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ loss_function.nll        │ GaussianNLLLoss                   │      0 │\n",
      "└────┴──────────────────────────┴───────────────────────────────────┴────────┘\n",
      "\u001b[1mTrainable params\u001b[0m: 7.4 M                                                         \n",
      "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \n",
      "\u001b[1mTotal params\u001b[0m: 7.4 M                                                             \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 29                                      \n",
      "Epoch 0: 100%|██| 171/171 [00:11<00:00, 15.38it/s, v_num=cdb1, train/loss=0.195]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▊                  | 1/22 [00:00<00:00, 61.89it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▋                 | 2/22 [00:00<00:00, 48.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▌                | 3/22 [00:00<00:00, 38.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▍               | 4/22 [00:00<00:00, 42.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▎              | 5/22 [00:00<00:00, 27.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|█████▏             | 6/22 [00:00<00:00, 30.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|██████             | 7/22 [00:00<00:00, 31.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▉            | 8/22 [00:00<00:00, 32.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▊           | 9/22 [00:00<00:00, 28.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|████████▏         | 10/22 [00:00<00:00, 29.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 11/22 [00:00<00:00, 30.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▊        | 12/22 [00:00<00:00, 30.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 13/22 [00:00<00:00, 29.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|███████████▍      | 14/22 [00:00<00:00, 30.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|████████████▎     | 15/22 [00:00<00:00, 30.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|█████████████     | 16/22 [00:00<00:00, 31.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▉    | 17/22 [00:00<00:00, 29.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|██████████████▋   | 18/22 [00:00<00:00, 30.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|███████████████▌  | 19/22 [00:00<00:00, 30.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▎ | 20/22 [00:00<00:00, 31.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████████████▏| 21/22 [00:00<00:00, 31.55it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 171/171 [00:12<00:00, 14.17it/s, v_num=cdb1, train/loss=0.195, \u001b[A\n",
      "Epoch 0: 100%|█| 171/171 [00:12<00:00, 14.17it/s, v_num=cdb1, train/loss=0.195, \u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 171/171 [00:12<00:00, 14.03it/s, v_num=cdb1, train/loss=0.195, \n"
     ]
    }
   ],
   "source": [
    "#train baseline model with the embedded sequence representations\n",
    "!python3 -m protml.apps.train experiment=supervised/train_emb \\\n",
    "    train_data=Data/data_sets/train_data_rand_emb.csv val_data=Data/data_sets/val_data_rand_emb.csv\\\n",
    "    embeddings=protT5/output/residue_embeddings.h5 trainer.max_epochs=1 \\\n",
    "    model.encoder.model_params.hidden_layer_sizes=[100,100,100,100,100] z_dim=1\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  vi) Adding the wt sequence as additional input  \n",
    "model source code: protml.models.dz_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-09 22:45:35,126][__main__][INFO] - Instantiating datamodule <protml.dataloaders.Sequence_WT_DataModule>\n",
      "[2023-05-09 22:45:44,509][__main__][INFO] - Seeding with 42\n",
      "Global seed set to 42\n",
      "[2023-05-09 22:45:44,513][__main__][INFO] - Instantiating model <protml.models.ENC_M_dz.factory>\n",
      "[2023-05-09 22:45:44,544][__main__][INFO] - Instantiating logger <pytorch_lightning.loggers.MLFlowLogger>\n",
      "[2023-05-09 22:45:45,208][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>\n",
      "[2023-05-09 22:45:45,214][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>\n",
      "[2023-05-09 22:45:45,215][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>\n",
      "[2023-05-09 22:45:45,216][__main__][INFO] - Instantiating trainer <pytorch_lightning.Trainer>\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[2023-05-09 22:45:45,299][__main__][INFO] - Starting training!\n",
      "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
      "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ encoder                  │ VariationalEncoder                │  184 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules   │ ModuleList                        │  184 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.0 │ Linear                            │  144 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.1 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.2 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.3 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.4 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_mean          │ Linear                            │    101 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_log_var       │ Linear                            │    101 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ measurement              │ Mave_Global_Epistasis_Measurement │    151 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ loss_function            │ GaussNLL_VAR_Loss                 │      1 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ loss_function.nll        │ GaussianNLLLoss                   │      0 │\n",
      "└────┴──────────────────────────┴───────────────────────────────────┴────────┘\n",
      "\u001b[1mTrainable params\u001b[0m: 184 K                                                         \n",
      "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \n",
      "\u001b[1mTotal params\u001b[0m: 184 K                                                             \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                       \n",
      "Epoch 0: 100%|█| 1049/1049 [01:14<00:00, 14.14it/s, v_num=1525, train/loss=0.510\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/132 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/132 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 1/132 [00:00<00:01, 68.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 2/132 [00:00<00:02, 45.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▍                 | 3/132 [00:00<00:03, 39.16it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 4/132 [00:00<00:03, 36.44it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 5/132 [00:00<00:03, 38.28it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▊                 | 6/132 [00:00<00:03, 33.74it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 7/132 [00:00<00:03, 34.21it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 8/132 [00:00<00:03, 34.99it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏                | 9/132 [00:00<00:03, 36.44it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 10/132 [00:00<00:03, 33.27it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 11/132 [00:00<00:03, 33.62it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 12/132 [00:00<00:03, 33.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 13/132 [00:00<00:03, 34.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 14/132 [00:00<00:03, 34.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 15/132 [00:00<00:03, 34.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 16/132 [00:00<00:03, 34.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 17/132 [00:00<00:03, 34.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▎              | 18/132 [00:00<00:03, 35.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 19/132 [00:00<00:03, 35.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 20/132 [00:00<00:03, 34.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 21/132 [00:00<00:03, 34.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 22/132 [00:00<00:03, 33.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 23/132 [00:00<00:03, 33.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 24/132 [00:00<00:03, 33.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 25/132 [00:00<00:03, 34.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 26/132 [00:00<00:03, 33.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 27/132 [00:00<00:03, 33.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 28/132 [00:00<00:03, 33.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 29/132 [00:00<00:03, 33.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▊             | 30/132 [00:00<00:02, 34.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 31/132 [00:00<00:02, 34.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 32/132 [00:00<00:02, 34.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 33/132 [00:00<00:02, 35.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 34/132 [00:00<00:02, 34.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 35/132 [00:00<00:02, 35.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 36/132 [00:01<00:02, 35.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 37/132 [00:01<00:02, 35.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 38/132 [00:01<00:02, 34.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 39/132 [00:01<00:02, 34.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▏           | 40/132 [00:01<00:02, 34.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 41/132 [00:01<00:02, 34.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 42/132 [00:01<00:02, 34.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▌           | 43/132 [00:01<00:02, 34.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 44/132 [00:01<00:02, 34.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 45/132 [00:01<00:02, 34.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 46/132 [00:01<00:02, 33.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 47/132 [00:01<00:02, 33.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 48/132 [00:01<00:02, 33.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 49/132 [00:01<00:02, 33.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 50/132 [00:01<00:02, 33.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 51/132 [00:01<00:02, 33.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 52/132 [00:01<00:02, 33.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 53/132 [00:01<00:02, 33.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 54/132 [00:01<00:02, 33.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████          | 55/132 [00:01<00:02, 33.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 56/132 [00:01<00:02, 33.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 57/132 [00:01<00:02, 33.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 58/132 [00:01<00:02, 33.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 59/132 [00:01<00:02, 33.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 60/132 [00:01<00:02, 33.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 61/132 [00:01<00:02, 33.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|███████▉         | 62/132 [00:01<00:02, 33.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 63/132 [00:01<00:02, 33.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 64/132 [00:01<00:02, 33.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 65/132 [00:01<00:01, 33.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 66/132 [00:01<00:01, 33.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 67/132 [00:01<00:01, 33.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 68/132 [00:02<00:01, 33.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 69/132 [00:02<00:01, 33.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|█████████        | 70/132 [00:02<00:01, 33.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 71/132 [00:02<00:01, 33.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 72/132 [00:02<00:01, 33.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 73/132 [00:02<00:01, 33.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 74/132 [00:02<00:01, 33.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 75/132 [00:02<00:01, 33.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 76/132 [00:02<00:01, 33.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▉       | 77/132 [00:02<00:01, 33.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 78/132 [00:02<00:01, 33.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 79/132 [00:02<00:01, 33.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 80/132 [00:02<00:01, 33.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 81/132 [00:02<00:01, 33.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 82/132 [00:02<00:01, 33.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 83/132 [00:02<00:01, 33.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▊      | 84/132 [00:02<00:01, 33.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▉      | 85/132 [00:02<00:01, 33.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████      | 86/132 [00:02<00:01, 33.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|███████████▏     | 87/132 [00:02<00:01, 33.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|███████████▎     | 88/132 [00:02<00:01, 33.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|███████████▍     | 89/132 [00:02<00:01, 33.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|███████████▌     | 90/132 [00:02<00:01, 33.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████▋     | 91/132 [00:02<00:01, 33.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▊     | 92/132 [00:02<00:01, 32.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▉     | 93/132 [00:02<00:01, 33.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████     | 94/132 [00:02<00:01, 32.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▏    | 95/132 [00:02<00:01, 32.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|████████████▎    | 96/132 [00:02<00:01, 33.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|████████████▍    | 97/132 [00:02<00:01, 32.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|████████████▌    | 98/132 [00:02<00:01, 32.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████▊    | 99/132 [00:03<00:01, 32.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████    | 100/132 [00:03<00:00, 32.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▏   | 101/132 [00:03<00:00, 32.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 102/132 [00:03<00:00, 32.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 103/132 [00:03<00:00, 32.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▌   | 104/132 [00:03<00:00, 33.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 105/132 [00:03<00:00, 32.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 106/132 [00:03<00:00, 32.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 107/132 [00:03<00:00, 32.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 108/132 [00:03<00:00, 32.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 109/132 [00:03<00:00, 33.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 110/132 [00:03<00:00, 33.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 111/132 [00:03<00:00, 33.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 112/132 [00:03<00:00, 33.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▋  | 113/132 [00:03<00:00, 33.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 114/132 [00:03<00:00, 33.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 115/132 [00:03<00:00, 32.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 116/132 [00:03<00:00, 32.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 117/132 [00:03<00:00, 32.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 118/132 [00:03<00:00, 31.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 119/132 [00:03<00:00, 31.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 120/132 [00:03<00:00, 31.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 121/132 [00:03<00:00, 31.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 122/132 [00:03<00:00, 31.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 123/132 [00:03<00:00, 31.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 124/132 [00:03<00:00, 31.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 125/132 [00:03<00:00, 31.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▎| 126/132 [00:03<00:00, 31.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 127/132 [00:03<00:00, 31.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 128/132 [00:04<00:00, 31.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 129/132 [00:04<00:00, 31.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▊| 130/132 [00:04<00:00, 31.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 131/132 [00:04<00:00, 32.03it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 1049/1049 [01:18<00:00, 13.31it/s, v_num=1525, train/loss=0.510\u001b[A\n",
      "Epoch 1:  23%|▏| 246/1049 [00:18<01:01, 12.98it/s, v_num=1525, train/loss=0.193,^CA\n"
     ]
    }
   ],
   "source": [
    "#Training on all sequences for referebnce. This representation is likely to be of more use together with a pretrained \n",
    "#embedding method, so that the latent space is already representative.   \n",
    "!python3 -m protml.apps.train experiment=supervised/train_dz \\\n",
    "    train_data=Data/data_sets/train_data_rand.csv val_data=Data/data_sets/val_data_rand.csv \\\n",
    "        trainer.max_epochs=2 model.encoder.model_params.hidden_layer_sizes=[100,100,100,100,100] z_dim=1 seed=42\\\n",
    "        callbacks.model_checkpoint.save_last=last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-10 01:02:47,160][__main__][INFO] - Instantiating datamodule <protml.dataloaders.Embeddings_WT_DataModule>\n",
      "[2023-05-10 01:02:47,187][__main__][INFO] - Seeding with 12345\n",
      "Global seed set to 12345\n",
      "[2023-05-10 01:02:47,189][__main__][INFO] - Instantiating model <protml.models.ENC_M_dz.factory>\n",
      "[2023-05-10 01:02:47,243][__main__][INFO] - Instantiating logger <pytorch_lightning.loggers.MLFlowLogger>\n",
      "[2023-05-10 01:02:47,340][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>\n",
      "[2023-05-10 01:02:47,343][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>\n",
      "[2023-05-10 01:02:47,344][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>\n",
      "[2023-05-10 01:02:47,344][__main__][INFO] - Instantiating trainer <pytorch_lightning.Trainer>\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[2023-05-10 01:02:47,381][__main__][INFO] - Starting training!\n",
      "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
      "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ encoder                  │ VariationalEncoder                │  7.4 M │\n",
      "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules   │ ModuleList                        │  7.4 M │\n",
      "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.0 │ Linear                            │  7.4 M │\n",
      "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.1 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.2 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.3 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ encoder.hidden_modules.4 │ Linear                            │ 10.1 K │\n",
      "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_mean          │ Linear                            │    101 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ encoder.fc_log_var       │ Linear                            │    101 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ measurement              │ Mave_Global_Epistasis_Measurement │    151 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ loss_function            │ GaussNLL_VAR_Loss                 │      1 │\n",
      "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ loss_function.nll        │ GaussianNLLLoss                   │      0 │\n",
      "└────┴──────────────────────────┴───────────────────────────────────┴────────┘\n",
      "\u001b[1mTrainable params\u001b[0m: 7.4 M                                                         \n",
      "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \n",
      "\u001b[1mTotal params\u001b[0m: 7.4 M                                                             \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 29                                      \n",
      "Epoch 0: 100%|██| 171/171 [00:18<00:00,  9.47it/s, v_num=af1b, train/loss=0.581]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▊                  | 1/22 [00:00<00:00, 26.90it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▋                 | 2/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▌                | 3/22 [00:00<00:00, 22.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▍               | 4/22 [00:00<00:00, 23.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▎              | 5/22 [00:00<00:00, 17.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|█████▏             | 6/22 [00:00<00:00, 18.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|██████             | 7/22 [00:00<00:00, 18.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▉            | 8/22 [00:00<00:00, 19.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▊           | 9/22 [00:00<00:00, 18.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|████████▏         | 10/22 [00:00<00:00, 18.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 11/22 [00:00<00:00, 19.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▊        | 12/22 [00:00<00:00, 19.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 13/22 [00:00<00:00, 19.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|███████████▍      | 14/22 [00:00<00:00, 19.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|████████████▎     | 15/22 [00:00<00:00, 19.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|█████████████     | 16/22 [00:00<00:00, 19.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▉    | 17/22 [00:00<00:00, 19.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|██████████████▋   | 18/22 [00:00<00:00, 19.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|███████████████▌  | 19/22 [00:00<00:00, 20.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▎ | 20/22 [00:00<00:00, 20.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████████████▏| 21/22 [00:01<00:00, 20.68it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 171/171 [00:19<00:00,  8.78it/s, v_num=af1b, train/loss=0.581, \u001b[A\n",
      "Epoch 0: 100%|█| 171/171 [00:19<00:00,  8.78it/s, v_num=af1b, train/loss=0.581, \u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 171/171 [00:19<00:00,  8.73it/s, v_num=af1b, train/loss=0.581, \n"
     ]
    }
   ],
   "source": [
    "# the same using T5 model embeddings\n",
    "!python3 -m protml.apps.train experiment=supervised/train_emb_wt \\\n",
    "    train_data=Data/data_sets/train_data_rand_emb.csv val_data=Data/data_sets/val_data_rand_emb.csv\\\n",
    "    embeddings=protT5/output/residue_embeddings.h5 wt_embeddings=protT5/output/wt_embeddings.h5 trainer.max_epochs=1 \\\n",
    "    model.encoder.model_params.hidden_layer_sizes=[100,100,100,100,100] z_dim=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
